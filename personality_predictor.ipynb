{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SETUP AND DATA LOADING\n",
    "# ==================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading data...\")\n",
    "train_df_orig = pd.read_csv('/kaggle/input/playground-series-s5e7/train.csv')\n",
    "test_df_orig = pd.read_csv(\"/kaggle/input/playground-series-s5e7/test.csv\")\n",
    "org_df = pd.read_csv('/kaggle/input/extrovert-vs-introvert-behavior-data/personality_datasert.csv')\n",
    "org_df_ = pd.read_csv('/kaggle/input/extrovert-vs-introvert-behavior-data/personality_dataset.csv')\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "# Combine training data\n",
    "train = pd.concat([train_df_orig, org_df, org_df_], ignore_index=True, axis=0)\n",
    "\n",
    "# Separate test ID and drop from test set\n",
    "test_id = test_df_orig['id']\n",
    "test = test_df_orig.drop(['id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb129946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATA PREPROCESSING\n",
    "# ==================================\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "# Map categorical string values to numbers\n",
    "train['Stage_fear'] = train['Stage_fear'].map({'Yes': 1, 'No': 0})\n",
    "test['Stage_fear'] = test['Stage_fear'].map({'Yes': 1, 'No': 0})\n",
    "train['Drained_after_socializing'] = train['Drained_after_socializing'].map({'Yes': 1, 'No': 0})\n",
    "test['Drained_after_socializing'] = test['Drained_after_socializing'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Map target variable 'Personality' to numbers\n",
    "train['Personality'] = train['Personality'].map({'Extrovert': 0, 'Introvert': 1})\n",
    "\n",
    "# Define feature columns and target variable\n",
    "feature_cols = test.columns\n",
    "X = train[feature_cols]\n",
    "y = train['Personality']\n",
    "\n",
    "# Impute missing values and scale features\n",
    "scaler = StandardScaler()\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_imputed = imputer.fit_transform(X_scaled)\n",
    "\n",
    "test_scaled = scaler.transform(test)\n",
    "test_imputed = imputer.transform(test_scaled)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_processed = pd.DataFrame(X_imputed, columns=feature_cols)\n",
    "test_processed = pd.DataFrame(test_imputed, columns=feature_cols)\n",
    "print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897258ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HYPERPARAMETER TUNING WITH OPTUNA\n",
    "# ==================================\n",
    "\n",
    "# Split data for tuning validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function with a reduced search space for faster tuning.\"\"\"\n",
    "    # -- Hyperparameters for XGBoost --\n",
    "    xgb_params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 200, 800),  # Reduced range\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 7),\n",
    "        'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "\n",
    "    # -- Hyperparameters for CatBoost --\n",
    "    cat_params = {\n",
    "        'iterations': trial.suggest_int('cat_iterations', 200, 800),  # Reduced range\n",
    "        'depth': trial.suggest_int('cat_depth', 4, 8),\n",
    "        'learning_rate': trial.suggest_float('cat_learning_rate', 0.01, 0.2, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('cat_l2_leaf_reg', 1, 10),\n",
    "    }\n",
    "\n",
    "    # -- Hyperparameters for LightGBM --\n",
    "    lgbm_params = {\n",
    "        'n_estimators': trial.suggest_int('lgbm_n_estimators', 200, 800),  # Reduced range\n",
    "        'num_leaves': trial.suggest_int('lgbm_num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('lgbm_learning_rate', 0.01, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('lgbm_subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('lgbm_colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    # Initialize models\n",
    "    xgb = XGBClassifier(**xgb_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    cat = CatBoostClassifier(**cat_params, random_seed=42, verbose=0)\n",
    "    lgbm = LGBMClassifier(**lgbm_params, random_state=42)\n",
    "\n",
    "    # Create the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=[('xgb', xgb), ('cat', cat), ('lgbm', lgbm)], voting='soft')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    val_preds = ensemble.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, val_preds)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "print(f\"\\nBest trial accuracy: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dcbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FINAL MODEL TRAINING (SUBMISSION STEPS REMOVED)\n",
    "# ===================================================\n",
    "print(\"\\nTraining final model on all data with best hyperparameters...\")\n",
    "\n",
    "# Extract the best parameters for each model\n",
    "best_params = study.best_params\n",
    "final_xgb_params = {k.replace('xgb_', ''): v for k, v in best_params.items() if k.startswith('xgb_')}\n",
    "final_cat_params = {k.replace('cat_', ''): v for k, v in best_params.items() if k.startswith('cat_')}\n",
    "final_lgbm_params = {k.replace('lgbm_', ''): v for k, v in best_params.items() if k.startswith('lgbm_')}\n",
    "\n",
    "# Initialize models with the best found hyperparameters\n",
    "final_xgb = XGBClassifier(**final_xgb_params, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "final_cat = CatBoostClassifier(**final_cat_params, random_seed=42, verbose=0)\n",
    "final_lgbm = LGBMClassifier(**final_lgbm_params, random_state=42)\n",
    "\n",
    "# Create the final voting ensemble\n",
    "final_ensemble = VotingClassifier(\n",
    "    estimators=[('xgb', final_xgb), ('cat', final_cat), ('lgbm', final_lgbm)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train the final model on the ENTIRE processed dataset\n",
    "final_ensemble.fit(X_processed, y)\n",
    "\n",
    "# Optional: Evaluate on the held-out validation set again\n",
    "val_preds_final = final_ensemble.predict(X_val)\n",
    "final_val_accuracy = accuracy_score(y_val, val_preds_final)\n",
    "print(f\"Final ensemble validation accuracy (re-evaluated): {final_val_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
